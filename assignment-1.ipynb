{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Problem A (25 points)\n",
    "\n",
    "__A1.__ In this problem, you will be working with the [Seinfeld Chronicles dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles). Create an account on [Kaggle](https://www.kaggle.com) and download the `scripts.csv` file from the dataset and move it into the `data` directory. Read the `data/scripts.csv` file as a text file line-by-line and examine the list you have loaded the data into. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Here, we read the file line by line and print them:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',Character,Dialogue,EpisodeNo,SEID,Season\\n', '0,JERRY,\"Do you know what this is all about? Do you know, why were here? To be out, this is out...and out is one of the single most enjoyable experiences of life. People...did you ever hear people talking about We should go out? This is what theyre talking about...this whole thing, were all out now, no one is home. Not one person here is home, were all out! There are people tryin to find us, they dont know where we are. (on an imaginary phone) Did you ring?, I cant find him. Where did he go? He didnt tell me where he was going. He must have gone out. You wanna go out you get ready, you pick out the clothes, right? You take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...Then youre standing around, whatta you do? You go We gotta be getting back. Once youre out, you wanna get back! You wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? Where ever you are in life, its my feeling, youve gotta go.\",1.0,S01E01,1.0\\n', '1,JERRY,\"(pointing at Georges shirt) See, to me, that button is in the worst possible spot. The second button literally makes or breaks the shirt, look at it. Its too high! Its in no-mans-land. You look like you live with your mother.\",1.0,S01E01,1.0\\n', '2,GEORGE,Are you through?,1.0,S01E01,1.0\\n', '3,JERRY,\"You do of course try on, when you buy?\",1.0,S01E01,1.0\\n', '4,GEORGE,\"Yes, it was purple, I liked it, I dont actually recall considering the buttons.\",1.0,S01E01,1.0\\n', '5,JERRY,\"Oh, you dont recall?\",1.0,S01E01,1.0\\n', '6,GEORGE,\"(on an imaginary microphone) Uh, no, not at this time.\",1.0,S01E01,1.0\\n', '7,JERRY,\"Well, senator, Id just like to know, what you knew and when you knew it.\",1.0,S01E01,1.0\\n', '8,CLAIRE,Mr. Seinfeld. Mr. Costanza.,1.0,S01E01,1.0\\n']\n"
     ]
    }
   ],
   "source": [
    "ContentOfSeinfeld = []\n",
    "with open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/scripts.csv\", \"r\") as file_handle:\n",
    "    for line in file_handle:\n",
    "        ContentOfSeinfeld.append(line)\n",
    "print(ContentOfSeinfeld[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ Is it possible to work with this data, simply splitting by a delimiter? Explain any complexity in the data's structured format that necessitates an established format-specific file reader. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>This data is formated as a list. The elements of the list are seperated with commas. In order to start working with this data we need and structured dataframe which consists of rows and columns. All the columns should be of a certain data type. For example, splitting by a delimiter like comma will lead to splitting the text itself at the comma points, which was not intended. Also, season ID is redundant since episode number and season number columns are existant. Additionally, all the info in one line counts as one single element of the list which we need to seperate into various columns in order to clean the data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JERRY', 'JERRY', 'GEORGE', 'JERRY', 'GEORGE', 'JERRY', 'GEORGE', 'JERRY', 'CLAIRE', 'GEORGE']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "Line_reader = csv.reader(open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/scripts.csv\", \"r\"))\n",
    "Content = {}\n",
    "i = 0\n",
    "for line in Line_reader:\n",
    "    if i: #newly initiated list is false\n",
    "        for j in range(len(header)):\n",
    "            heading = header[j]\n",
    "            datum = line[j]\n",
    "            Content[heading].append(datum)\n",
    "    else:\n",
    "        header = line\n",
    "        for heading in header:\n",
    "            Content[heading] = []\n",
    "    i += 1\n",
    "Characters = Content['Character']\n",
    "type(Characters)\n",
    "print(Characters[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "myset = set(Characters)\n",
    "Speakers_Num = len(myset)\n",
    "print(Speakers_Num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Thus, we have 1639 characters talking in the TV series. Although, we can see that this number is not accurate since there are few instances that the speaker name is repeated in another form such as voice-over or for example Jerry(shouting) can be a seperate character here which is not accurate.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ Count the dialogue entries for the four major characters, \"JERRY\", \"GEORGE\", \"ELAINE\", and \"KRAMER\", using a dictionary (you are not allowed to use the Counter data structure for any component of this problem). (2 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of dialogue enteries for JERRY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14786"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in Characters if i == \"JERRY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of dialogue enteries for GEORGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9708"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in Characters if i == \"GEORGE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of dialogue enteries for ELAINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7983"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in Characters if i == \"ELAINE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of dialogue enteries for KRAMER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6664"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in Characters if i == \"KRAMER\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ Count the number of words spoken by each of the main characters using a dictionary. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>First, I searched for each characters Speech lines seperatly and then we can count the words for each of them.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count for JERRY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "JerrySpeech = [Content['Dialogue'][i] for i in range(len(Content['Character'])) if Content['Character'][i] == \"JERRY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147389"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wordsjerry = [len(sentence.split()) for sentence in JerrySpeech]\n",
    "jerry_words = sum(num_wordsjerry)\n",
    "jerry_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count for GEORGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeorgeSpeech = [Content['Dialogue'][i] for i in range(len(Content['Character'])) if Content['Character'][i] == \"GEORGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107029"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wordsgeorge = [len(sentence.split()) for sentence in GeorgeSpeech]\n",
    "george_words = sum(num_wordsgeorge)\n",
    "george_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count for ELAINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElaineSpeech = [Content['Dialogue'][i] for i in range(len(Content['Character'])) if Content['Character'][i] == \"ELAINE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74634"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wordselaine = [len(sentence.split()) for sentence in ElaineSpeech]\n",
    "elaine_words = sum(num_wordselaine)\n",
    "elaine_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count for KRAMER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "KramerSpeech = [Content['Dialogue'][i] for i in range(len(Content['Character'])) if Content['Character'][i] == \"KRAMER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70299"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wordskramer = [len(sentence.split()) for sentence in KramerSpeech]\n",
    "kramer_words = sum(num_wordskramer)\n",
    "kramer_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ Count how many times each word is spoken by the main characters using a dictionary, then sort these word counts in descending order, i.e. from the most commonly spoken words to least. [__Hint__: You can use either a lambda function or a list comprehension to do this.] (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most spoken words of Jerry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4708), ('I', 4665), ('you', 3622), ('a', 3294), ('to', 3059), ('of', 1518), ('in', 1304), ('and', 1282), ('You', 1188), ('is', 1181), ('that', 1150), ('it', 1060), ('with', 927), ('on', 904), (\"I'm\", 884), (\"don't\", 832), ('have', 803), ('What', 799), ('this', 794), ('was', 793), ('my', 772), ('like', 751), ('do', 749), ('for', 746), ('just', 704), ('know', 693), ('what', 682), ('get', 682), ('not', 672), ('your', 620), ('be', 619), ('me', 596), ('Oh,', 586), ('are', 583), ('about', 571), ('at', 565), ('he', 560), ('Well,', 557), ('out', 554), ('it.', 517), ('got', 502), ('think', 473), ('we', 461), ('his', 451), ('up', 451), ('Yeah,', 450), (\"it's\", 445), ('So', 426), ('all', 412), ('they', 410)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "JerryDict = {}\n",
    "for sentence in JerrySpeech:\n",
    "    for word in re.split('\\s', sentence): # split with whitespace\n",
    "        try:\n",
    "            JerryDict[word] += 1\n",
    "        except KeyError:\n",
    "            JerryDict[word] = 1\n",
    "JerryDict = sorted(JerryDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(JerryDict[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most spoken words by George:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 3946), ('the', 3136), ('a', 2457), ('to', 2287), ('you', 2269), ('of', 1033), ('in', 923), ('and', 922), ('You', 879), (\"I'm\", 823), ('is', 809), ('it', 786), ('that', 763), ('my', 706), ('have', 646), ('was', 644), ('with', 631), ('for', 605), ('me', 600), ('this', 573), ('like', 543), ('not', 532), ('What', 520), ('do', 518), ('on', 515), ('just', 497), (\"don't\", 485), ('know', 479), ('be', 471), ('get', 460), ('what', 441), (\"It's\", 415), ('about', 404), ('are', 383), ('at', 378), ('all', 371), ('think', 371), ('got', 367), ('she', 361), ('Oh,', 339), ('out', 339), ('Well,', 328), ('it.', 326), ('your', 324), ('he', 323), ('her', 312), ('we', 306), ('his', 301), ('up', 298), (\"it's\", 289)]\n"
     ]
    }
   ],
   "source": [
    "GeorgeDict = {}\n",
    "for sentence in GeorgeSpeech:\n",
    "    for word in re.split('\\s', sentence): # split with whitespace\n",
    "        try:\n",
    "            GeorgeDict[word] += 1\n",
    "        except KeyError:\n",
    "            GeorgeDict[word] = 1\n",
    "GeorgeDict = sorted(GeorgeDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(GeorgeDict[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most spoken words by Elaine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 2604), ('the', 1797), ('you', 1713), ('a', 1557), ('to', 1538), ('and', 748), ('is', 711), ('of', 634), ('that', 603), ('You', 598), ('in', 588), (\"I'm\", 495), ('it', 493), ('Oh,', 490), ('this', 464), ('have', 458), ('just', 444), ('my', 438), ('he', 429), ('was', 416), ('What', 398), (\"don't\", 384), ('with', 376), ('for', 368), ('me', 360), ('on', 358), ('do', 334), ('know', 329), ('are', 308), ('at', 308), ('not', 305), ('Well,', 303), ('what', 290), ('like', 282), ('be', 275), ('get', 269), ('her', 265), ('got', 261), ('your', 259), ('about', 258), ('Yeah,', 246), ('think', 245), ('so', 240), ('we', 238), ('out', 230), (\"It's\", 220), ('up', 216), (\"it's\", 209), ('No,', 200), ('it.', 196)]\n"
     ]
    }
   ],
   "source": [
    "ElaineDict = {}\n",
    "for sentence in ElaineSpeech:\n",
    "    for word in re.split('\\s', sentence): # split with whitespace\n",
    "        try:\n",
    "            ElaineDict[word] += 1\n",
    "        except KeyError:\n",
    "            ElaineDict[word] = 1\n",
    "ElaineDict = sorted(ElaineDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(ElaineDict[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most spoken words by Kramer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2208), ('I', 2155), ('a', 1560), ('you', 1534), ('to', 1382), ('and', 797), ('of', 685), ('in', 607), (\"I'm\", 551), ('You', 516), ('Well,', 512), ('it', 501), ('that', 498), ('on', 452), ('is', 424), ('my', 415), ('Yeah,', 396), ('Oh,', 391), ('got', 384), ('for', 375), ('this', 355), ('with', 343), ('at', 317), ('was', 313), ('get', 313), (\"don't\", 309), ('me', 302), ('your', 291), ('know', 290), ('have', 285), ('be', 283), ('like', 278), ('just', 267), ('what', 263), ('out', 259), ('his', 256), ('do', 255), ('gonna', 250), ('are', 246), ('he', 245), ('all', 240), (\"It's\", 239), ('up', 227), ('Hey,', 226), ('it.', 225), (\"it's\", 220), ('Jerry,', 217), ('we', 201), ('not', 197), ('know,', 195)]\n"
     ]
    }
   ],
   "source": [
    "KramerDict = {}\n",
    "for sentence in KramerSpeech:\n",
    "    for word in re.split('\\s', sentence): # split with whitespace\n",
    "        try:\n",
    "            KramerDict[word] += 1\n",
    "        except KeyError:\n",
    "            KramerDict[word] = 1\n",
    "KramerDict = sorted(KramerDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(KramerDict[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ Load the `data/stop-words.txt` file into a list. Find the 10 most common words for each of the main characters that are not in this list of stop words. Put these most common words in a dictionary data strucutre. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the text file and making a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "stop_words = []\n",
    "with open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/stop-words.txt\", \"r\") as file_handle:\n",
    "    for line in file_handle:\n",
    "        stop_words.extend([i for i in line.split()])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Here, we find the top 10 used words for each character that were not in  the list.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jerry's top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsOfJerry = [JerryDict[i][0].lower() for i in range(len(JerryDict))]\n",
    "#print(WordsOfJerry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidWordsJerry = [x for x in WordsOfJerry if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm\", 'like', 'know', 'get', 'oh,', 'well,', 'it.', 'got', 'think', 'yeah,']\n"
     ]
    }
   ],
   "source": [
    "#Jerry\n",
    "Top10Jerry = ValidWordsJerry[:10]\n",
    "print(Top10Jerry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### George's top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsOfGeorge = [GeorgeDict[i][0].lower() for i in range(len(GeorgeDict))]\n",
    "#print(WordsOfGeorge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidWordsGeorge = [x for x in WordsOfGeorge if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm\", 'like', 'know', 'get', 'think', 'got', 'oh,', 'well,', 'it.', 'see']\n"
     ]
    }
   ],
   "source": [
    "#George\n",
    "Top10George = ValidWordsGeorge[:10]\n",
    "print(Top10George)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elaine's top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsOfElaine = [ElaineDict[i][0].lower() for i in range(len(ElaineDict))]\n",
    "#print(WordsOfElaine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidWordsElaine = [x for x in WordsOfElaine if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm\", 'oh,', 'know', 'well,', 'like', 'get', 'got', 'yeah,', 'think', 'no,']\n"
     ]
    }
   ],
   "source": [
    "#Elaine\n",
    "Top10Elaine = ValidWordsElaine[:10]\n",
    "print(Top10Elaine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kramer's top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsOfKramer = [KramerDict[i][0].lower() for i in range(len(KramerDict))]\n",
    "#print(WordsOfKramer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidWordsKramer = [x for x in WordsOfKramer if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm\", 'well,', 'yeah,', 'oh,', 'got', 'get', 'know', 'like', 'gonna', 'hey,']\n"
     ]
    }
   ],
   "source": [
    "#Kramer\n",
    "Top10Kramer = ValidWordsKramer[:10]\n",
    "print(Top10Kramer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B (25 points)\n",
    "\n",
    "__B1.__ You will be using part of the [Goodbooks 10k dataset](https://github.com/zygmuntz/goodbooks-10k) for this problem. Read the `data/goodreads-books.csv` file into a list. Create a dictionary for each book in the list that contains these fields: authors, original title, original publication year, average rating, and ratings count. (You should convert average rating and ratings count into `float` and `int` types.) Put all these metadata dictionaries into a list. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['book_id', 'goodreads_book_id', 'best_book_id', 'work_id', 'books_count', 'isbn', 'isbn13', 'authors', 'original_publication_year', 'original_title', 'title', 'language_code', 'average_rating', 'ratings_count', 'work_ratings_count', 'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'image_url', 'small_image_url'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LineOfBooks = csv.reader(open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/goodreads-books.csv\", \"r\"))\n",
    "Books = {}\n",
    "i = 0\n",
    "for line in LineOfBooks:\n",
    "    if i: #newly initiated list is false\n",
    "        for j in range(len(header)):\n",
    "            heading = header[j]\n",
    "            datum = line[j]\n",
    "            Books[heading].append(datum)\n",
    "    else:\n",
    "        header = line\n",
    "        for heading in header:\n",
    "            Books[heading] = []\n",
    "    i += 1\n",
    "Books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Authors = Books['authors']\n",
    "OriginalTitle = Books['original_title']\n",
    "OriginalPublicationYear = Books['original_publication_year']\n",
    "RatingsCounttemp = Books['ratings_count']\n",
    "RatingsCount = [int(i) for i in RatingsCounttemp]\n",
    "AverageRatingtemp = Books['average_rating']\n",
    "AverageRating = [float(i) for i in AverageRatingtemp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_title': 'The Hunger Games', 'authors': 'Suzanne Collins', 'original_publication_year': '2008.0', 'ratings_count': 4780653, 'average_rating': 4.34}, {'original_title': \"Harry Potter and the Philosopher's Stone\", 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '1997.0', 'ratings_count': 4602479, 'average_rating': 4.44}, {'original_title': 'Twilight', 'authors': 'Stephenie Meyer', 'original_publication_year': '2005.0', 'ratings_count': 3866839, 'average_rating': 3.57}, {'original_title': 'To Kill a Mockingbird', 'authors': 'Harper Lee', 'original_publication_year': '1960.0', 'ratings_count': 3198671, 'average_rating': 4.25}, {'original_title': 'The Great Gatsby', 'authors': 'F. Scott Fitzgerald', 'original_publication_year': '1925.0', 'ratings_count': 2683664, 'average_rating': 3.89}, {'original_title': 'The Fault in Our Stars', 'authors': 'John Green', 'original_publication_year': '2012.0', 'ratings_count': 2346404, 'average_rating': 4.26}, {'original_title': 'The Hobbit or There and Back Again', 'authors': 'J.R.R. Tolkien', 'original_publication_year': '1937.0', 'ratings_count': 2071616, 'average_rating': 4.25}, {'original_title': 'The Catcher in the Rye', 'authors': 'J.D. Salinger', 'original_publication_year': '1951.0', 'ratings_count': 2044241, 'average_rating': 3.79}, {'original_title': 'Angels & Demons ', 'authors': 'Dan Brown', 'original_publication_year': '2000.0', 'ratings_count': 2001311, 'average_rating': 3.85}, {'original_title': 'Pride and Prejudice', 'authors': 'Jane Austen', 'original_publication_year': '1813.0', 'ratings_count': 2035490, 'average_rating': 4.24}]\n"
     ]
    }
   ],
   "source": [
    "All_the_book = []\n",
    "\n",
    "for i in range (len(OriginalTitle)):\n",
    "    All_the_book.append(dict([(\"original_title\", Books[\"original_title\"][i])]))\n",
    "\n",
    "for i in range (len(OriginalTitle)):\n",
    "    All_the_book[i][\"authors\"] = Authors[i]\n",
    "    All_the_book[i][\"original_publication_year\"] = OriginalPublicationYear[i]\n",
    "    All_the_book[i][\"ratings_count\"] = RatingsCount[i]\n",
    "    All_the_book[i][\"average_rating\"] = AverageRating[i]\n",
    "\n",
    "print(All_the_book[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ Write a function to sort this list of book metadata in descending order of average rating. The function should take the list of metadata dictionaries as an input argument. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortingBooks(metadata):\n",
    "    SortedList = sorted(metadata, key=lambda k: k['average_rating'], reverse=True)\n",
    "    return SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_the_books_sorted = SortingBooks(All_the_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ Update the function to take two arguments: the list, and an integer value for minimum ratings count. The function should now sort _and_ filter the list, returning a list of books sorted by average rating that have been rated by more than a specified number of users. You can use three different approaches: loops, comprehensions, and the built-in `filter()` function (look up documentation and examples). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilteredSortingBooks(metadata, n):\n",
    "    FilteredList = list(filter(lambda k: k['ratings_count'] > n, metadata))\n",
    "    SortedList = sorted(FilteredList, key=lambda k: k['average_rating'], reverse=True)\n",
    "    return SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_title': 'Harry Potter and the Deathly Hallows', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2007.0', 'ratings_count': 1746574, 'average_rating': 4.61}, {'original_title': \"The Wise Man's Fear\", 'authors': 'Patrick Rothfuss', 'original_publication_year': '2011.0', 'ratings_count': 245686, 'average_rating': 4.57}, {'original_title': 'The Name of the Wind', 'authors': 'Patrick Rothfuss', 'original_publication_year': '2007.0', 'ratings_count': 400101, 'average_rating': 4.55}, {'original_title': 'Harry Potter and the Half-Blood Prince', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2005.0', 'ratings_count': 1678823, 'average_rating': 4.54}, {'original_title': 'A Storm of Swords', 'authors': 'George R.R. Martin', 'original_publication_year': '2000.0', 'ratings_count': 469022, 'average_rating': 4.54}, {'original_title': 'The Nightingale', 'authors': 'Kristin Hannah', 'original_publication_year': '2015.0', 'ratings_count': 253606, 'average_rating': 4.54}, {'original_title': 'Harry Potter and the Prisoner of Azkaban', 'authors': 'J.K. Rowling, Mary GrandPré, Rufus Beck', 'original_publication_year': '1999.0', 'ratings_count': 1832823, 'average_rating': 4.53}, {'original_title': 'Harry Potter and the Goblet of Fire', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2000.0', 'ratings_count': 1753043, 'average_rating': 4.53}, {'original_title': 'The Return of the King', 'authors': 'J.R.R. Tolkien', 'original_publication_year': '1955.0', 'ratings_count': 463959, 'average_rating': 4.51}, {'original_title': 'The Last Olympian', 'authors': 'Rick Riordan', 'original_publication_year': '2009.0', 'ratings_count': 397500, 'average_rating': 4.5}]\n"
     ]
    }
   ],
   "source": [
    "FilteredSortedAllTheBooks = FilteredSortingBooks(All_the_book, 200000)\n",
    "print(FilteredSortedAllTheBooks[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus:__ Use all three approaches to write three different versions of the function. See if you can condense your code into a single line for some of the approaches! (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoopsSortingBooks(metadata, n):\n",
    "    QualifiedBooks = []\n",
    "    for EachBook in metadata:\n",
    "        if EachBook['ratings_count'] > n:\n",
    "            QualifiedBooks.append(EachBook)\n",
    "    SortedList = sorted(QualifiedBooks, key=lambda k: k['average_rating'], reverse=True)\n",
    "    return SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_title': 'Harry Potter and the Deathly Hallows', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2007.0', 'ratings_count': 1746574, 'average_rating': 4.61}, {'original_title': \"The Wise Man's Fear\", 'authors': 'Patrick Rothfuss', 'original_publication_year': '2011.0', 'ratings_count': 245686, 'average_rating': 4.57}]\n"
     ]
    }
   ],
   "source": [
    "LoopsSortedAllTheBooks = LoopsSortingBooks(All_the_book, 200000)\n",
    "print(LoopsSortedAllTheBooks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComprehensionSortingBooks(metadata, n):\n",
    "    QualifiedBooks = [EachBook for EachBook in metadata if EachBook['ratings_count'] > n]\n",
    "    SortedList = sorted(QualifiedBooks, key=lambda k: k['average_rating'], reverse=True)\n",
    "    return SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_title': 'Harry Potter and the Deathly Hallows', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2007.0', 'ratings_count': 1746574, 'average_rating': 4.61}, {'original_title': \"The Wise Man's Fear\", 'authors': 'Patrick Rothfuss', 'original_publication_year': '2011.0', 'ratings_count': 245686, 'average_rating': 4.57}]\n"
     ]
    }
   ],
   "source": [
    "ComprehensionSortedAllTheBooks = ComprehensionSortingBooks(All_the_book, 200000)\n",
    "print(ComprehensionSortedAllTheBooks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3: filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilteredSortingBooks(metadata, n):\n",
    "    SortedList = sorted(\n",
    "        list(filter(lambda k: k['ratings_count'] > n, metadata)), key=lambda k: k['average_rating'], reverse=True)\n",
    "    return SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_title': 'Harry Potter and the Deathly Hallows', 'authors': 'J.K. Rowling, Mary GrandPré', 'original_publication_year': '2007.0', 'ratings_count': 1746574, 'average_rating': 4.61}, {'original_title': \"The Wise Man's Fear\", 'authors': 'Patrick Rothfuss', 'original_publication_year': '2011.0', 'ratings_count': 245686, 'average_rating': 4.57}]\n"
     ]
    }
   ],
   "source": [
    "FilteredSortedAllTheBooks = FilteredSortingBooks(All_the_book, 200000)\n",
    "print(FilteredSortedAllTheBooks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ Why is using a function for this task prudent? What do you think is an acceptable minimum ratings count? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>It is prudent to use a function with n as the argument for the minimum rating count because it will exclude the titles with very few rating counts which usually has unrealistic ratings. In another word, the average rating is valid with enough data points. For high confidence levels, the rating count should be higher. For example, for 95% confidence level(1.96 is the coefficient for 95% confidence level) and the marginal error being less than 100 books, for this data set, where we have 10000 book titles, the minimum acceptable rating count is:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n \\geq (1.96(10000)/100)^2 = 38416 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem C (50 points)\n",
    "\n",
    "This problem deals with finding \"pangrams\" in text. A pangram is a sentence containing all 26 letters of the alphabet. `x` and `y` in the cell below are example sentences, `x` is a pangram, `y` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Jim quickly realized that the beautiful gowns are expensive.\"\n",
    "y = \"This sentence is most certainly not a pangram.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C1.__ Define a generator function, `indices()`, that takes a string as input and outputs the index numbers where a letter occurs for the first time in the string. [__Hint:__ you can compare letters like numbers. For example, `char >= \"a\"` is a valid conditional statement. You can use this to check whether characters in a string are letters.] (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>When I solved this problem I was not aware of the .index method, so I did not used that in here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    Alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    IndexList = []\n",
    "    for i in Alphabet:\n",
    "        checkforletter = [pos for pos, char in enumerate(input_string) if char == i]\n",
    "        if checkforletter:\n",
    "            IndexList.append(min(checkforletter))\n",
    "        else:\n",
    "            IndexList.append(\"NA\")\n",
    "        Index_number = IndexList\n",
    "    yield Index_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 6,\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 8,\n",
       " 'NA',\n",
       " 11,\n",
       " 'NA',\n",
       " 0,\n",
       " 3,\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 9,\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 'NA',\n",
       " 1,\n",
       " 'NA']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in indices(\"My name is kiana\")][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ Define a function, `verify()`, that takes a string as input and uses the `indices()` function to check if the string is a pangram. The output should be boolean `True` or `False`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(String_entry):\n",
    "    String_entry = String_entry.lower()\n",
    "    Index_number = [n for n in indices(String_entry)][0]\n",
    "    if \"NA\" in Index_number:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"the quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"This sentence is most certainly not a pangram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3:__ Write a version of `verify()` named `tiny_verify()` that performs the check in a single line of code, without using `indices()`. [__Hint:__ Use a comprehension.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_verify(inputString):\n",
    "    return not set('abcdefghijklmnopqrstuvwxyz') - set(inputString.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_verify(\"the quick brown fox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ Modify the `verify()` function to figure out which letters (if any) are missing from a purported pangram. This version should return the list of missing letters instead of a boolean value. [__Hint:__ You can get a string containing all the letters of the alphabet by importing `ascii_lowercase` from the `string` module.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify2(inputString):\n",
    "    inpuString = inputString.lower()\n",
    "    Alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    Index_number = [n for n in indices(inputString)][0]\n",
    "    ListOfMissingLetters = []\n",
    "    for i in range(len(Alphabet)):\n",
    "        if Index_number[i] == \"NA\":\n",
    "            ListOfMissingLetters.append(Alphabet[i])  \n",
    "    return ListOfMissingLetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'j',\n",
       " 'l',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'z']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify2(\"My name is kiana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify2(\"the quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C5.__ Load and iterated through the collected [list of pangrams](http://clagnut.com/blog/2380/) in `data/pangrams.txt` line by line and determine if they are actually pangrams. Print out any lines that are not actually pangrams, and also the letters that are missing. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show mangled quartz flip vibe exactly.\n",
      "\n",
      "['j', 'k']\n",
      "Unamazingly, this six-word pangram is questionable!\n",
      "\n",
      "['c', 'f', 'j', 'k', 'v']\n"
     ]
    }
   ],
   "source": [
    "FailedPangrams = []\n",
    "with open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/pangrams.txt\", \"r\") as file_handle:\n",
    "    for line in file_handle:\n",
    "        if verify(line) == False:\n",
    "            FailedPangrams.append(line)\n",
    "            print(line)\n",
    "            print(verify2(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C6:__ Use the output from the `verify()` function to fix the failed pangrams, and verify that you have fixed them. (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "FixedPangrams = []\n",
    "for line in FailedPangrams:\n",
    "    for missingchar in verify2(line):\n",
    "        line = line + missingchar\n",
    "    FixedPangrams.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show mangled quartz flip vibe exactly.\n",
      "jk together is a True pangram\n",
      "Unamazingly, this six-word pangram is questionable!\n",
      "cfjkv together is a True pangram\n"
     ]
    }
   ],
   "source": [
    "for element in FixedPangrams:\n",
    "    print(element, \"together is a\", verify(element), \"pangram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C7.__ In the cell below are provided some information about a set of books. Create a data object that holds the book numbers and titles associated to each authors's name. Write this out as a JSON file in the `data/books/` directory using the following schema. (5 points)\n",
    "\n",
    "`\n",
    "books = {\n",
    "    AuthorName: {\n",
    "        BookNumber: BookTitle,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 84.txt; Frankenstein, or the Modern Prometheus; Mary Wollstonecraft (Godwin) Shelley\n",
    "# 98.txt; A Tale of Two Cities; Charles Dickens \n",
    "# 161.txt; Sense and Sensibility; Jane Austen\n",
    "# 730.txt; Oliver Twist or the Parish Boy's Progress; Charles Dickens\n",
    "# 768.txt; Wuthering Heights; Emily Brontë\n",
    "# 1322.txt; Leaves of Grass; Walt Whitman\n",
    "# 1342.txt; Pride and Prejudice; Jane Austen\n",
    "# 1400.txt; Great Expectations; Charles Dickens\n",
    "# 2701.txt; Moby Dick; or the Whale; Herman Melville\n",
    "# 4300.txt; Ulysses; James Joyce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "books = {}  \n",
    "books['Mary Wollstonecraft (Godwin) Shelley'] = []  \n",
    "books['Mary Wollstonecraft (Godwin) Shelley'].append({  \n",
    "    'BookNumber': '84',\n",
    "    'BookTitle': 'Frankenstein, or the Modern Prometheus'\n",
    "})\n",
    "books['Charles Dickens'] = []\n",
    "books['Charles Dickens'].append({  \n",
    "    'BookNumber': '98',\n",
    "    'BookTitle': 'A Tale of Two Cities'\n",
    "})\n",
    "books['Charles Dickens'].append({  \n",
    "    'BookNumber': '730',\n",
    "    'BookTitle': \"Oliver Twist or the Parish Boy's Progress\"\n",
    "})\n",
    "books['Charles Dickens'].append({  \n",
    "    'BookNumber': '1400',\n",
    "    'BookTitle': \"Great Expectations\"\n",
    "})\n",
    "books['Jane Austen'] = []\n",
    "books['Jane Austen'].append({  \n",
    "    'BookNumber': '161',\n",
    "    'BookTitle': \"Sense and Sensibility\"\n",
    "})\n",
    "books['Jane Austen'].append({  \n",
    "    'BookNumber': '1342',\n",
    "    'BookTitle': \"Pride and Prejudice\"\n",
    "})\n",
    "books['Emily Brontë'] = []\n",
    "books['Jane Austen'].append({  \n",
    "    'BookNumber': '768',\n",
    "    'BookTitle': \"Wuthering Heights\"\n",
    "})\n",
    "books['Walt Whitman'] = []\n",
    "books['Walt Whitman'].append({  \n",
    "    'BookNumber': '1322',\n",
    "    'BookTitle': \"Leaves of Grass\"\n",
    "})\n",
    "books['Herman Melville'] = []\n",
    "books['Herman Melville'].append({  \n",
    "    'BookNumber': '2701',\n",
    "    'BookTitle': \"Moby Dick, or the Whale\"\n",
    "})\n",
    "books['James Joyce'] = []\n",
    "books['James Joyce'].append({  \n",
    "    'BookNumber': '4300',\n",
    "    'BookTitle': \"Ulysses\"\n",
    "})\n",
    "with open('/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/books/BooksInfo.txt', 'w'\n",
    "         ) as outfile:  \n",
    "    json.dump(books, outfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C8.__ Write a function, `get_pangrams()`, that takes a book number and outputs a list of the book's pangram sentences and the total number of sentences in the book. You will need to use the `re` (regular expressions) module to split the book text into sentences using the `re.split(pattern, string)` function. The pattern you will need is `\"[\\.\\?\\!][^a-zA-Z]\"`. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>In this part we are seperating the sentences in each book roughly. It means that we are ignoring the error caused by more comlex sentences which need some data cleaning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_pangrams(Book_Number):\n",
    "    address = \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/books/\"\n",
    "    address = address + str(Book_Number) + \".txt\"\n",
    "    pangram = []\n",
    "    sentences = []\n",
    "    with open(address, \"r\") as file_handle:\n",
    "        allthetext = file_handle.read()\n",
    "    sentenceswithnewline = re.split(\"[\\.\\?\\!][^a-zA-Z]\", allthetext)\n",
    "    for i in sentenceswithnewline:\n",
    "        sentences.append(i.strip('\\n'))\n",
    "    for elements in sentences:\n",
    "        pangram.append(verify(str(elements)))\n",
    "    NumberOfPangrams = sum(pangram)\n",
    "    return([NumberOfPangrams, len(sentences)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C9.__ Determine who is the pangrammiest author and what the pangrammiest book is, as determined by pangrams per sentence. [__Hint:__ Use `defaultdict`s to create \"pangrams by author\" and \"pangrams by book\" objects.] (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the pangrams per sentence for each book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsandpangrams = [(\"Mary Wollstonecraft (Godwin) Shelley\", get_pangrams(84)[0]/get_pangrams(84)[1]), \n",
    "        (\"Charles Dickens\", get_pangrams(98)[0]/get_pangrams(98)[1]), \n",
    "        (\"Jane Austen\", get_pangrams(161)[0]/get_pangrams(161)[1]), \n",
    "        (\"Charles Dickens\", get_pangrams(730)[0]/get_pangrams(730)[1]), \n",
    "        (\"Emily Brontë\", get_pangrams(768)[0]/get_pangrams(768)[1]), \n",
    "        (\"Walt Whitman\", get_pangrams(1322)[0]/get_pangrams(1322)[1]), \n",
    "        (\"Jane Austen\", get_pangrams(1342)[0]/get_pangrams(1342)[1]), \n",
    "        (\"Charles Dickens\", get_pangrams(1400)[0]/get_pangrams(1400)[1]), \n",
    "        (\"Herman Melville\", get_pangrams(2701)[0]/get_pangrams(2701)[1]), \n",
    "        (\"James Joyce\", get_pangrams(4300)[0]/get_pangrams(4300)[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the pangrams per sentence for each author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'Mary Wollstonecraft (Godwin) Shelley': [0.0], 'Charles Dickens': [0.00011811953697141507, 9.492168960607498e-05, 0.0], 'Jane Austen': [0.0, 0.00042005040604872583], 'Emily Brontë': [0.00013795006207752792], 'Walt Whitman': [0.00233160621761658], 'Herman Melville': [0.00038387715930902113], 'James Joyce': [0.00044682752457551384]})\n"
     ]
    }
   ],
   "source": [
    "pangrams_by_author = defaultdict(list)\n",
    "\n",
    "for authors, authorsandpangrams in authorsandpangrams:\n",
    "    pangrams_by_author[authors].append(authorsandpangrams)    \n",
    "print(pangrams_by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mary Wollstonecraft (Godwin) Shelley': 0.0, 'Charles Dickens': 0.00021304122657749006, 'Jane Austen': 0.00042005040604872583, 'Emily Brontë': 0.00013795006207752792, 'Walt Whitman': 0.00233160621761658, 'Herman Melville': 0.00038387715930902113, 'James Joyce': 0.00044682752457551384}\n"
     ]
    }
   ],
   "source": [
    "summed = {k: sum(v) for (k, v) in pangrams_by_author.items()}\n",
    "print(summed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the pangramiest authors from the most to the least:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walt Whitman 0.00233160621761658\n",
      "James Joyce 0.00044682752457551384\n",
      "Jane Austen 0.00042005040604872583\n",
      "Herman Melville 0.00038387715930902113\n",
      "Charles Dickens 0.00021304122657749006\n",
      "Emily Brontë 0.00013795006207752792\n",
      "Mary Wollstonecraft (Godwin) Shelley 0.0\n"
     ]
    }
   ],
   "source": [
    "for w in sorted(summed, key=summed.get, reverse=True):\n",
    "  print(w, summed[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the pangrams per sentence for each book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksandpangrams = [(\"Frankenstein, or the Modern Prometheus\", get_pangrams(84)[0]/get_pangrams(84)[1]), \n",
    "        (\"A Tale of Two Cities\", get_pangrams(98)[0]/get_pangrams(98)[1]), \n",
    "        (\"Sense and Sensibility\", get_pangrams(161)[0]/get_pangrams(161)[1]), \n",
    "        (\"Oliver Twist or the Parish Boy's Progress\", get_pangrams(730)[0]/get_pangrams(730)[1]), \n",
    "        (\"Wuthering Heights\", get_pangrams(768)[0]/get_pangrams(768)[1]), \n",
    "        (\"Leaves of Grass\", get_pangrams(1322)[0]/get_pangrams(1322)[1]), \n",
    "        (\"Pride and Prejudice\", get_pangrams(1342)[0]/get_pangrams(1342)[1]), \n",
    "        (\"Great Expectations\", get_pangrams(1400)[0]/get_pangrams(1400)[1]), \n",
    "        (\"Moby Dick, or the Whale\", get_pangrams(2701)[0]/get_pangrams(2701)[1]), \n",
    "        (\"Ulysses\", get_pangrams(4300)[0]/get_pangrams(4300)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'Frankenstein, or the Modern Prometheus': [0.0], 'A Tale of Two Cities': [0.00011811953697141507], 'Sense and Sensibility': [0.0], \"Oliver Twist or the Parish Boy's Progress\": [9.492168960607498e-05], 'Wuthering Heights': [0.00013795006207752792], 'Leaves of Grass': [0.00233160621761658], 'Pride and Prejudice': [0.00042005040604872583], 'Great Expectations': [0.0], 'Moby Dick, or the Whale': [0.00038387715930902113], 'Ulysses': [0.00044682752457551384]})\n"
     ]
    }
   ],
   "source": [
    "pangrams_by_book = defaultdict(list)\n",
    "\n",
    "for title, booksandpangrams in booksandpangrams:\n",
    "    pangrams_by_book[title].append(booksandpangrams)\n",
    "    \n",
    "print(pangrams_by_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the pangramiest book from the most to the least:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaves of Grass [0.00233160621761658]\n",
      "Ulysses [0.00044682752457551384]\n",
      "Pride and Prejudice [0.00042005040604872583]\n",
      "Moby Dick, or the Whale [0.00038387715930902113]\n",
      "Wuthering Heights [0.00013795006207752792]\n",
      "A Tale of Two Cities [0.00011811953697141507]\n",
      "Oliver Twist or the Parish Boy's Progress [9.492168960607498e-05]\n",
      "Frankenstein, or the Modern Prometheus [0.0]\n",
      "Sense and Sensibility [0.0]\n",
      "Great Expectations [0.0]\n"
     ]
    }
   ],
   "source": [
    "for w in sorted(pangrams_by_book, key=pangrams_by_book.get, reverse=True):\n",
    "  print(w, pangrams_by_book[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus:__ Print out the most efficient pangram and its author and book, as determined by fewest characters. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_pangrams():\n",
    "    BookNumber = [84, 98, 161, 730, 768, 1322, 1342, 1400, 2701, 4300]\n",
    "    efficient = {}\n",
    "    for booknumber in BookNumber:\n",
    "        address = \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/books/\"\n",
    "        address = address + str(booknumber) + \".txt\"\n",
    "        sentences = []\n",
    "        pansent = []\n",
    "        efficient[booknumber] = []\n",
    "        with open(address, \"r\") as file_handle:\n",
    "            allthetext = file_handle.read()\n",
    "            sentenceswithnewline = re.split(\"[\\.\\?\\!][^a-zA-Z]\", allthetext)\n",
    "        for i in sentenceswithnewline:\n",
    "            sentences.append(i.strip('\\n'))\n",
    "        for elements in sentences:\n",
    "            if verify(str(elements)):\n",
    "                pansent.append(str(elements))\n",
    "        for each in pansent:\n",
    "            letters = 0\n",
    "            for i in range(len(each)):\n",
    "                if each[i].isalpha():\n",
    "                    letters += 1\n",
    "            efficient[booknumber].append(letters)\n",
    "        if efficient[booknumber]:\n",
    "            efficient[booknumber] = min(efficient[booknumber])\n",
    "        else:\n",
    "            efficient.pop(booknumber)\n",
    "    return(efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{98: 389, 730: 223, 768: 379, 1322: 417, 1342: 310, 2701: 391, 4300: 260}\n"
     ]
    }
   ],
   "source": [
    "d = efficient_pangrams()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_efficient_Pangram_characters = min(d.values())\n",
    "type(most_efficient_Pangram_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    }
   ],
   "source": [
    "BookNumber = [k for k,v in d.items() if v == most_efficient_Pangram_characters]\n",
    "print(BookNumber[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pansent = []\n",
    "sentences = []\n",
    "with open(\n",
    "    \"/Users/Kianamon/Dropbox/DSCI511/KianaIsEditing/Assignments/assignment-1/data/books/730.txt\", \"r\") as file_handle:\n",
    "        allthetext = file_handle.read()\n",
    "        sentenceswithnewline = re.split(\"[\\.\\?\\!][^a-zA-Z]\", allthetext)\n",
    "        for i in sentenceswithnewline:\n",
    "            sentences.append(i.strip('\\n'))\n",
    "        for elements in sentences:\n",
    "            if verify(str(elements)):\n",
    "                pansent.append(str(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': \"Oliver Twist or the Parish Boy's Progress\", 'Author': 'Charles Dickens', 'NumberOfChars': 223, 'EfficientPangram': 'At least half a\\ndozen more were severally drawn forth from the same box, and surveyed\\nwith equal pleasure; besides rings, brooches, bracelets, and other\\narticles of jewellery, of such magnificent materials, and costly\\nworkmanship, that Oliver had no idea, even of their names'}\n"
     ]
    }
   ],
   "source": [
    "MostEfficientPangramInTheBooks = {}\n",
    "MostEfficientPangramInTheBooks['Title'] = \"Oliver Twist or the Parish Boy's Progress\"\n",
    "MostEfficientPangramInTheBooks['Author'] = 'Charles Dickens'\n",
    "MostEfficientPangramInTheBooks['NumberOfChars'] = most_efficient_Pangram_characters\n",
    "MostEfficientPangramInTheBooks['EfficientPangram'] = pansent[0].strip('\\n') \n",
    "print(MostEfficientPangramInTheBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
