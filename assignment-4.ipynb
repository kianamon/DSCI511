{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 511\n",
    "\n",
    "## Assignment 4 _(120 points)_\n",
    "## E-reader enhancement with Wikification and Project Gutenberg\n",
    "For this assignment your work will setup a complete pipeline for the backend of an e-reader enhancement. Specifically, you'll use an API service known as Wikifier to embed hyperlink references to encyclopedia articles within the content areas of html open-domain eBooks.\n",
    "\n",
    "### A. _(40 points)_ Getting the relevant API's in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries in use:\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse, urllib.request\n",
    "import csv \n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "import csv, json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython.core.display import HTML\n",
    "import emoji \n",
    "import os\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(5 points)_ Throughout this problem, we're going to be working with the Wikifier API, which is \"a web service that takes a text document as input and annotates it with links to relevant Wikipedia concepts\". So, before we can begin with the actual work, it's necessary to request access to this API at the following link: http://wikifier.org/info.html. Read this documentation and familiarize yourself with it, and make sure to register for a `USER_KEY`.\n",
    "Store your `USER_KEY` in the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_KEY = 'xnmoymphovtdrmqxkvazxarlzhxzwr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(10 points)_ Now, we'd like to incorporate some Project Gutenberg data. You may recall that each book in the Project Gutenberg collection has a unique ID associated with it. We'd like to write a function that takes as input the ID of a specific Project Gutenberg book and then downloads and consequently stores a full HTML version (with images) of the book. \\[__Hint__: To sort out the url structure find and observe the URLs of a few (HTML-format) books on Gutenberg: https://www.gutenberg.org/ and generalize the pattern in your code, bringing in the book's ID number as a an argument into the final URL as needed.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GutenbergDLer(GutenbergID):\n",
    "    BookURL = \"http://www.gutenberg.org/files/\"+GutenbergID+\"/\"+GutenbergID+\"-h/\"+GutenbergID+\"-h.htm\"\n",
    "    html_Book = requests.get(BookURL)\n",
    "    return html_Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(5 points)_ Let's use the function you wrote in the previous part to download a book! Let's grab _Alice in Wonderland_ (ID number `19033`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_Alice = GutenbergDLer(\"19033\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_file(GutenbergID):\n",
    "    htmlBook = GutenbergDLer(GutenbergID)\n",
    "    Html_file= open(\"BookID\"+GutenbergID+\".htm\",\"w\")\n",
    "    Html_file.write(htmlBook.text)\n",
    "    Html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_html_file(\"19033\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(15 points)_ In addition to accessing books, we're going to have to make requests to Wikifier, ultimately to the paragraphs of _Alice in Wonderland_. Create a function that takes a block of text as input, and outputs the JSON response given by the Wikification API (per the following specifications) interpreted as a Python object (deserialized). __Important__: For full credit you must read the Wikifier docs: http://wikifier.org/info.html; specifically, you must determine how to build a URL that sets the `'applyPageRankSqThreshold'` field to `True`, the corresponding `'pageRankSqThreshold'` field to `0.8`, and the language field to `'en'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wikify(text):\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", \"en\"),\n",
    "        (\"userKey\", USER_KEY),\n",
    "        (\"pageRankSqThreshold\", \"%g\" % 0.8), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"200\"), (\"nWordsToIgnoreFromList\", \"200\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "        ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout = 60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    return response    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrexelWikify = Wikify(\n",
    "    \"Drexel University is a private research university with its main campus\"+\n",
    "    \" located in the University City neighborhood of Philadelphia, Pennsylvania\"+\n",
    "    \", United States. It was founded in 1891 by Anthony J. Drexel, a noted financier\"+\n",
    "    \" and philanthropist. Founded as Drexel Institute of Art, Science, and Industry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(DrexelWikify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotations', 'spaces', 'words', 'normWords', 'ranges', 'minPageRank', 'maxDfThreshold', 'altLabelSetsUsed', 'linkSourcesAvailable', 'timeTotalMs', 'timeAnnotationMs', 'timeBuildJsonMs', 'incIdxTimestampUtc', 'fnRegIdx', 'fnWikiData', 'settings', 'language'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DrexelWikify.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drexel', 'University', 'is', 'a', 'private', 'research', 'university', 'with', 'its', 'main', 'campus', 'located', 'in', 'the', 'University', 'City', 'neighborhood', 'of', 'Philadelphia', ',', 'Pennsylvania', ',', 'United', 'States', '.', 'It', 'was', 'founded', 'in', '1891', 'by', 'Anthony', 'J', '.', 'Drexel', ',', 'a', 'noted', 'financier', 'and', 'philanthropist', '.', 'Founded', 'as', 'Drexel', 'Institute', 'of', 'Art', ',', 'Science', ',', 'and', 'Industry']\n"
     ]
    }
   ],
   "source": [
    "print(DrexelWikify[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '', ' ', '', ' ', ' ', '', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '', ' ', '', ' ', ' ', ' ', ' ', ' ', '', ' ', ' ', ' ', ' ', ' ', ' ', '', ' ', '', ' ', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "print(DrexelWikify[\"spaces\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dbPediaIri': 'http://dbpedia.org/resource/Drexel_University',\n",
      "  'dbPediaTypes': ['Agent',\n",
      "                   'Place',\n",
      "                   'Organisation',\n",
      "                   'University',\n",
      "                   'EducationalInstitution'],\n",
      "  'lang': 'en',\n",
      "  'pageRank': 0.04405086906225279,\n",
      "  'secLang': 'en',\n",
      "  'secTitle': 'Drexel University',\n",
      "  'secUrl': 'http://en.wikipedia.org/wiki/Drexel_University',\n",
      "  'support': [{'chFrom': 0,\n",
      "               'chTo': 16,\n",
      "               'entropy': 0.2412578517720322,\n",
      "               'pMentionGivenSurface': 0.675933280381255,\n",
      "               'pageRank': 0.01713757418971146,\n",
      "               'prbConfidence': 0.7074427084630097,\n",
      "               'wFrom': 0,\n",
      "               'wTo': 1},\n",
      "              {'chFrom': 256,\n",
      "               'chTo': 271,\n",
      "               'entropy': 0,\n",
      "               'pMentionGivenSurface': 0.3877551020408163,\n",
      "               'pageRank': 0.009831120942758526,\n",
      "               'prbConfidence': 1,\n",
      "               'wFrom': 44,\n",
      "               'wTo': 45},\n",
      "              {'chFrom': 256,\n",
      "               'chTo': 278,\n",
      "               'entropy': 0,\n",
      "               'pMentionGivenSurface': 0.04545454545454546,\n",
      "               'pageRank': 0.001152451976543464,\n",
      "               'prbConfidence': 1,\n",
      "               'wFrom': 44,\n",
      "               'wTo': 47},\n",
      "              {'chFrom': 256,\n",
      "               'chTo': 301,\n",
      "               'entropy': 0,\n",
      "               'pMentionGivenSurface': 0.3333333333333333,\n",
      "               'pageRank': 0.008451314494652066,\n",
      "               'prbConfidence': 1,\n",
      "               'wFrom': 44,\n",
      "               'wTo': 52}],\n",
      "  'supportLen': 4,\n",
      "  'title': 'Drexel University',\n",
      "  'url': 'http://en.wikipedia.org/wiki/Drexel_University',\n",
      "  'wikiDataClasses': [{'enLabel': 'university', 'itemId': 'Q3918'},\n",
      "                      {'enLabel': 'private not-for-profit educational '\n",
      "                                  'institution',\n",
      "                       'itemId': 'Q23002054'},\n",
      "                      {'enLabel': 'higher education institution',\n",
      "                       'itemId': 'Q38723'},\n",
      "                      {'enLabel': 'academic institution', 'itemId': 'Q4671277'},\n",
      "                      {'enLabel': 'private educational institution',\n",
      "                       'itemId': 'Q23002042'},\n",
      "                      {'enLabel': 'nonprofit organization',\n",
      "                       'itemId': 'Q163740'},\n",
      "                      {'enLabel': 'educational institution',\n",
      "                       'itemId': 'Q2385804'},\n",
      "                      {'enLabel': 'organization', 'itemId': 'Q43229'},\n",
      "                      {'enLabel': 'facility', 'itemId': 'Q13226383'},\n",
      "                      {'enLabel': 'institution', 'itemId': 'Q178706'},\n",
      "                      {'enLabel': 'educational organization',\n",
      "                       'itemId': 'Q5341295'},\n",
      "                      {'enLabel': 'social group', 'itemId': 'Q874405'},\n",
      "                      {'enLabel': 'agent', 'itemId': 'Q24229398'},\n",
      "                      {'enLabel': 'group of humans', 'itemId': 'Q16334295'},\n",
      "                      {'enLabel': 'physical object', 'itemId': 'Q223557'},\n",
      "                      {'enLabel': 'location', 'itemId': 'Q17334923'},\n",
      "                      {'enLabel': 'geographical object', 'itemId': 'Q618123'},\n",
      "                      {'enLabel': 'system', 'itemId': 'Q58778'},\n",
      "                      {'enLabel': 'unit of analysis', 'itemId': 'Q7887142'},\n",
      "                      {'enLabel': 'entity', 'itemId': 'Q35120'},\n",
      "                      {'enLabel': 'living thing group', 'itemId': 'Q16334298'},\n",
      "                      {'enLabel': 'concrete object', 'itemId': 'Q4406616'},\n",
      "                      {'enLabel': 'physical system', 'itemId': 'Q1454986'},\n",
      "                      {'enLabel': 'physical substance', 'itemId': 'Q28732711'},\n",
      "                      {'enLabel': 'abstract object', 'itemId': 'Q7184903'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q6671777'},\n",
      "                      {'enLabel': 'research subject', 'itemId': 'Q4330518'},\n",
      "                      {'enLabel': 'unit', 'itemId': 'Q2198779'},\n",
      "                      {'enLabel': 'group', 'itemId': 'Q16887380'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q517966'},\n",
      "                      {'enLabel': 'object', 'itemId': 'Q488383'},\n",
      "                      {'enLabel': 'object of science', 'itemId': 'Q30060700'},\n",
      "                      {'enLabel': 'concept', 'itemId': 'Q151885'},\n",
      "                      {'enLabel': 'mental representation',\n",
      "                       'itemId': 'Q2145290'},\n",
      "                      {'enLabel': 'representation', 'itemId': 'Q1272626'}],\n",
      "  'wikiDataItemId': 'Q603034'},\n",
      " {'dbPediaIri': '',\n",
      "  'dbPediaTypes': [],\n",
      "  'lang': 'en',\n",
      "  'pageRank': 0.01530170015304907,\n",
      "  'secLang': 'en',\n",
      "  'secTitle': 'Research university',\n",
      "  'secUrl': 'http://en.wikipedia.org/wiki/Research_university',\n",
      "  'support': [{'chFrom': 31,\n",
      "               'chTo': 38,\n",
      "               'entropy': 1.131697482854864,\n",
      "               'pMentionGivenSurface': 0.009491269679534151,\n",
      "               'pageRank': 0.0002406411150458959,\n",
      "               'prbConfidence': 0.1836431391508375,\n",
      "               'wFrom': 5,\n",
      "               'wTo': 5},\n",
      "              {'chFrom': 31,\n",
      "               'chTo': 49,\n",
      "               'entropy': 0.5879266667579027,\n",
      "               'pMentionGivenSurface': 0.3348082595870207,\n",
      "               'pageRank': 0.008488709691531057,\n",
      "               'prbConfidence': 0.3107072569605561,\n",
      "               'wFrom': 5,\n",
      "               'wTo': 6}],\n",
      "  'supportLen': 2,\n",
      "  'title': 'Research university',\n",
      "  'url': 'http://en.wikipedia.org/wiki/Research_university',\n",
      "  'wikiDataClasses': [],\n",
      "  'wikiDataItemId': 'Q15936437'},\n",
      " {'dbPediaIri': 'http://dbpedia.org/resource/Philadelphia',\n",
      "  'dbPediaTypes': ['City',\n",
      "                   'Settlement',\n",
      "                   'PopulatedPlace',\n",
      "                   'Place',\n",
      "                   'AdministrativeRegion',\n",
      "                   'Region'],\n",
      "  'lang': 'en',\n",
      "  'pageRank': 0.03413312728727867,\n",
      "  'secLang': 'en',\n",
      "  'secTitle': 'Philadelphia',\n",
      "  'secUrl': 'http://en.wikipedia.org/wiki/Philadelphia',\n",
      "  'support': [{'chFrom': 119,\n",
      "               'chTo': 130,\n",
      "               'entropy': 0.7609945396277396,\n",
      "               'pMentionGivenSurface': 0.2616514415225804,\n",
      "               'pageRank': 0.006633895860859174,\n",
      "               'prbConfidence': 0.2662515034202353,\n",
      "               'wFrom': 18,\n",
      "               'wTo': 18},\n",
      "              {'chFrom': 119,\n",
      "               'chTo': 131,\n",
      "               'entropy': 1,\n",
      "               'pMentionGivenSurface': 7.381435689241557e-05,\n",
      "               'pageRank': 1.871485032954877e-06,\n",
      "               'prbConfidence': 0.8401560503720767,\n",
      "               'wFrom': 18,\n",
      "               'wTo': 19},\n",
      "              {'chFrom': 119,\n",
      "               'chTo': 144,\n",
      "               'entropy': 0.09587672952724037,\n",
      "               'pMentionGivenSurface': 0.669687637483502,\n",
      "               'pageRank': 0.01697922251266086,\n",
      "               'prbConfidence': 0.2505926974864564,\n",
      "               'wFrom': 18,\n",
      "               'wTo': 20},\n",
      "              {'chFrom': 119,\n",
      "               'chTo': 159,\n",
      "               'entropy': 0,\n",
      "               'pMentionGivenSurface': 0.0078125,\n",
      "               'pageRank': 0.0001980776834684078,\n",
      "               'prbConfidence': 1,\n",
      "               'wFrom': 18,\n",
      "               'wTo': 23},\n",
      "              {'chFrom': 133,\n",
      "               'chTo': 144,\n",
      "               'entropy': 0.3059203807573613,\n",
      "               'pMentionGivenSurface': 0.2388409744288092,\n",
      "               'pageRank': 0.006055560567321056,\n",
      "               'prbConfidence': 0.2945271417470611,\n",
      "               'wFrom': 20,\n",
      "               'wTo': 20}],\n",
      "  'supportLen': 5,\n",
      "  'title': 'Philadelphia',\n",
      "  'url': 'http://en.wikipedia.org/wiki/Philadelphia',\n",
      "  'wikiDataClasses': [{'enLabel': 'city of Pennsylvania',\n",
      "                       'itemId': 'Q21010817'},\n",
      "                      {'enLabel': 'city with millions of inhabitants',\n",
      "                       'itemId': 'Q1637706'},\n",
      "                      {'enLabel': 'city of the United States',\n",
      "                       'itemId': 'Q1093829'},\n",
      "                      {'enLabel': 'local government in Pennsylvania',\n",
      "                       'itemId': 'Q6664406'},\n",
      "                      {'enLabel': 'big city', 'itemId': 'Q1549591'},\n",
      "                      {'enLabel': 'city', 'itemId': 'Q515'},\n",
      "                      {'enLabel': 'municipal corporation in the United States',\n",
      "                       'itemId': 'Q3327870'},\n",
      "                      {'enLabel': 'definitions of a city by country',\n",
      "                       'itemId': 'Q15253706'},\n",
      "                      {'enLabel': 'human settlement', 'itemId': 'Q486972'},\n",
      "                      {'enLabel': 'administrative territorial entity of the '\n",
      "                                  'United States',\n",
      "                       'itemId': 'Q852446'},\n",
      "                      {'enLabel': 'administrative territorial entity',\n",
      "                       'itemId': 'Q56061'},\n",
      "                      {'enLabel': 'political territorial entity',\n",
      "                       'itemId': 'Q1048835'},\n",
      "                      {'enLabel': 'community', 'itemId': 'Q177634'},\n",
      "                      {'enLabel': 'municipality', 'itemId': 'Q15284'},\n",
      "                      {'enLabel': 'municipal corporation',\n",
      "                       'itemId': 'Q2097994'},\n",
      "                      {'enLabel': 'geographic region', 'itemId': 'Q82794'},\n",
      "                      {'enLabel': 'geographic location', 'itemId': 'Q2221906'},\n",
      "                      {'enLabel': 'geographical object', 'itemId': 'Q618123'},\n",
      "                      {'enLabel': 'human-geographic territorial entity',\n",
      "                       'itemId': 'Q15642541'},\n",
      "                      {'enLabel': 'administrative territorial entity of a '\n",
      "                                  'single country',\n",
      "                       'itemId': 'Q15916867'},\n",
      "                      {'enLabel': 'political entity', 'itemId': 'Q16562419'},\n",
      "                      {'enLabel': 'social group', 'itemId': 'Q874405'},\n",
      "                      {'enLabel': 'local government', 'itemId': 'Q6501447'},\n",
      "                      {'enLabel': 'location', 'itemId': 'Q17334923'},\n",
      "                      {'enLabel': 'physical object', 'itemId': 'Q223557'},\n",
      "                      {'enLabel': 'territorial entity', 'itemId': 'Q1496967'},\n",
      "                      {'enLabel': 'artificial entity', 'itemId': 'Q16686448'},\n",
      "                      {'enLabel': 'entity', 'itemId': 'Q35120'},\n",
      "                      {'enLabel': 'group of humans', 'itemId': 'Q16334295'},\n",
      "                      {'enLabel': 'system', 'itemId': 'Q58778'},\n",
      "                      {'enLabel': 'unit of analysis', 'itemId': 'Q7887142'},\n",
      "                      {'enLabel': 'government', 'itemId': 'Q7188'},\n",
      "                      {'enLabel': 'concrete object', 'itemId': 'Q4406616'},\n",
      "                      {'enLabel': 'physical system', 'itemId': 'Q1454986'},\n",
      "                      {'enLabel': 'physical substance', 'itemId': 'Q28732711'},\n",
      "                      {'enLabel': 'object', 'itemId': 'Q488383'},\n",
      "                      {'enLabel': 'living thing group', 'itemId': 'Q16334298'},\n",
      "                      {'enLabel': 'abstract object', 'itemId': 'Q7184903'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q6671777'},\n",
      "                      {'enLabel': 'research subject', 'itemId': 'Q4330518'},\n",
      "                      {'enLabel': 'unit', 'itemId': 'Q2198779'},\n",
      "                      {'enLabel': 'political organization',\n",
      "                       'itemId': 'Q7210356'},\n",
      "                      {'enLabel': 'authority organ', 'itemId': 'Q895526'},\n",
      "                      {'enLabel': 'object of science', 'itemId': 'Q30060700'},\n",
      "                      {'enLabel': 'group', 'itemId': 'Q16887380'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q517966'},\n",
      "                      {'enLabel': 'concept', 'itemId': 'Q151885'},\n",
      "                      {'enLabel': 'organization', 'itemId': 'Q43229'},\n",
      "                      {'enLabel': 'authority', 'itemId': 'Q174834'},\n",
      "                      {'enLabel': 'political power', 'itemId': 'Q2101636'},\n",
      "                      {'enLabel': 'mental representation',\n",
      "                       'itemId': 'Q2145290'},\n",
      "                      {'enLabel': 'agent', 'itemId': 'Q24229398'},\n",
      "                      {'enLabel': 'power', 'itemId': 'Q25107'},\n",
      "                      {'enLabel': 'representation', 'itemId': 'Q1272626'},\n",
      "                      {'enLabel': 'power', 'itemId': 'Q18340964'}],\n",
      "  'wikiDataItemId': 'Q1345'},\n",
      " {'dbPediaIri': 'http://dbpedia.org/resource/Pennsylvania',\n",
      "  'dbPediaTypes': ['PopulatedPlace',\n",
      "                   'Place',\n",
      "                   'AdministrativeRegion',\n",
      "                   'Region',\n",
      "                   'State'],\n",
      "  'lang': 'en',\n",
      "  'pageRank': 0.01720562750394245,\n",
      "  'secLang': 'en',\n",
      "  'secTitle': 'Pennsylvania',\n",
      "  'secUrl': 'http://en.wikipedia.org/wiki/Pennsylvania',\n",
      "  'support': [{'chFrom': 133,\n",
      "               'chTo': 159,\n",
      "               'entropy': 0,\n",
      "               'pMentionGivenSurface': 0.01050420168067227,\n",
      "               'pageRank': 0.0002663229357558424,\n",
      "               'prbConfidence': 1,\n",
      "               'wFrom': 20,\n",
      "               'wTo': 23}],\n",
      "  'supportLen': 1,\n",
      "  'title': 'Pennsylvania',\n",
      "  'url': 'http://en.wikipedia.org/wiki/Pennsylvania',\n",
      "  'wikiDataClasses': [{'enLabel': 'state of the United States',\n",
      "                       'itemId': 'Q35657'},\n",
      "                      {'enLabel': 'administrative territorial entity of the '\n",
      "                                  'United States',\n",
      "                       'itemId': 'Q852446'},\n",
      "                      {'enLabel': 'federated state', 'itemId': 'Q107390'},\n",
      "                      {'enLabel': 'first-level administrative country '\n",
      "                                  'subdivision',\n",
      "                       'itemId': 'Q10864048'},\n",
      "                      {'enLabel': 'administrative territorial entity of a '\n",
      "                                  'single country',\n",
      "                       'itemId': 'Q15916867'},\n",
      "                      {'enLabel': 'state', 'itemId': 'Q7275'},\n",
      "                      {'enLabel': 'administrative territorial entity of a '\n",
      "                                  'specific level',\n",
      "                       'itemId': 'Q1799794'},\n",
      "                      {'enLabel': 'administrative territorial entity',\n",
      "                       'itemId': 'Q56061'},\n",
      "                      {'enLabel': 'political territorial entity',\n",
      "                       'itemId': 'Q1048835'},\n",
      "                      {'enLabel': 'organization', 'itemId': 'Q43229'},\n",
      "                      {'enLabel': 'community', 'itemId': 'Q177634'},\n",
      "                      {'enLabel': 'human-geographic territorial entity',\n",
      "                       'itemId': 'Q15642541'},\n",
      "                      {'enLabel': 'political entity', 'itemId': 'Q16562419'},\n",
      "                      {'enLabel': 'social group', 'itemId': 'Q874405'},\n",
      "                      {'enLabel': 'agent', 'itemId': 'Q24229398'},\n",
      "                      {'enLabel': 'group of humans', 'itemId': 'Q16334295'},\n",
      "                      {'enLabel': 'territorial entity', 'itemId': 'Q1496967'},\n",
      "                      {'enLabel': 'artificial entity', 'itemId': 'Q16686448'},\n",
      "                      {'enLabel': 'entity', 'itemId': 'Q35120'},\n",
      "                      {'enLabel': 'system', 'itemId': 'Q58778'},\n",
      "                      {'enLabel': 'unit of analysis', 'itemId': 'Q7887142'},\n",
      "                      {'enLabel': 'living thing group', 'itemId': 'Q16334298'},\n",
      "                      {'enLabel': 'geographic region', 'itemId': 'Q82794'},\n",
      "                      {'enLabel': 'geographic location', 'itemId': 'Q2221906'},\n",
      "                      {'enLabel': 'object', 'itemId': 'Q488383'},\n",
      "                      {'enLabel': 'abstract object', 'itemId': 'Q7184903'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q6671777'},\n",
      "                      {'enLabel': 'research subject', 'itemId': 'Q4330518'},\n",
      "                      {'enLabel': 'unit', 'itemId': 'Q2198779'},\n",
      "                      {'enLabel': 'group', 'itemId': 'Q16887380'},\n",
      "                      {'enLabel': 'structure', 'itemId': 'Q517966'},\n",
      "                      {'enLabel': 'geographical object', 'itemId': 'Q618123'},\n",
      "                      {'enLabel': 'location', 'itemId': 'Q17334923'},\n",
      "                      {'enLabel': 'concept', 'itemId': 'Q151885'},\n",
      "                      {'enLabel': 'concrete object', 'itemId': 'Q4406616'},\n",
      "                      {'enLabel': 'physical object', 'itemId': 'Q223557'},\n",
      "                      {'enLabel': 'mental representation',\n",
      "                       'itemId': 'Q2145290'},\n",
      "                      {'enLabel': 'physical system', 'itemId': 'Q1454986'},\n",
      "                      {'enLabel': 'physical substance', 'itemId': 'Q28732711'},\n",
      "                      {'enLabel': 'representation', 'itemId': 'Q1272626'},\n",
      "                      {'enLabel': 'object of science', 'itemId': 'Q30060700'}],\n",
      "  'wikiDataItemId': 'Q1400'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(DrexelWikify[\"annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drexel University (http://en.wikipedia.org/wiki/Drexel_University)\n",
      "Research university (http://en.wikipedia.org/wiki/Research_university)\n",
      "Philadelphia (http://en.wikipedia.org/wiki/Philadelphia)\n",
      "Pennsylvania (http://en.wikipedia.org/wiki/Pennsylvania)\n"
     ]
    }
   ],
   "source": [
    "for annotation in Wikify(\n",
    "    \"Drexel University is a private research university with its main campus\"+\n",
    "    \" located in the University City neighborhood of Philadelphia, Pennsylvania\"+\n",
    "    \", United States. It was founded in 1891 by Anthony J. Drexel, a noted financier\"+\n",
    "    \" and philanthropist. Founded as Drexel Institute of Art, Science, and Industry\")[\"annotations\"]:\n",
    "    print(\"%s (%s)\" % (annotation[\"title\"], annotation[\"url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(5 points)_ Check to make sure your function works! Load the small block of text provided under `./data/example_text.txt`, and apply your `Wikify()` function. Print out the result. Does everything look correct, according to the docs? Discuss the output in the response box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>The output of the function is a dictionary that has annotation key. We can print the Wiki pages by looping over all the annotations and we can extract the URL for each page. One of the links is not correct; the phrase \"end of\" is linked to a wiki page called Moonsault!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = open(\"./data/example_text.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Hey smilin' strange\\n\"\n",
      " \"You're lookin' happily deranged\\n\"\n",
      " 'Could you settle to shoot me?\\n'\n",
      " 'Or have you picked your target yet?\\n'\n",
      " 'Hey Sandy\\n'\n",
      " \"Don't you talk back,\\n\"\n",
      " 'Hey Sandy\\n'\n",
      " 'Four feet away\\n'\n",
      " \"End of speech, it's the end of the day\\n\"\n",
      " \"We was only funnin'\\n\"\n",
      " \"But guiltily I thought you had it comin'\\n\")\n"
     ]
    }
   ],
   "source": [
    "pprint(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikifiedExample = Wikify(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'altLabelSetsUsed': [],\n",
      " 'annotations': [{'dbPediaIri': 'http://dbpedia.org/resource/Hey_Sandy',\n",
      "                  'dbPediaTypes': ['Person', 'Agent', 'Artist'],\n",
      "                  'lang': 'en',\n",
      "                  'pageRank': 0.1080767492300702,\n",
      "                  'secLang': 'en',\n",
      "                  'secTitle': 'Hey Sandy',\n",
      "                  'secUrl': 'http://en.wikipedia.org/wiki/Hey_Sandy',\n",
      "                  'support': [{'chFrom': 118,\n",
      "                               'chTo': 126,\n",
      "                               'entropy': 0,\n",
      "                               'pMentionGivenSurface': 0.3846153846153846,\n",
      "                               'pageRank': 0.04410080617528433,\n",
      "                               'prbConfidence': 1,\n",
      "                               'wFrom': 24,\n",
      "                               'wTo': 25},\n",
      "                              {'chFrom': 149,\n",
      "                               'chTo': 157,\n",
      "                               'entropy': 0,\n",
      "                               'pMentionGivenSurface': 0.3846153846153846,\n",
      "                               'pageRank': 0.04410080617528433,\n",
      "                               'prbConfidence': 1,\n",
      "                               'wFrom': 31,\n",
      "                               'wTo': 32}],\n",
      "                  'supportLen': 2,\n",
      "                  'title': 'Hey Sandy',\n",
      "                  'url': 'http://en.wikipedia.org/wiki/Hey_Sandy',\n",
      "                  'wikiDataClasses': [],\n",
      "                  'wikiDataItemId': 'Q16574914'},\n",
      "                 {'dbPediaIri': 'http://dbpedia.org/resource/Moonsault',\n",
      "                  'dbPediaTypes': [],\n",
      "                  'lang': 'en',\n",
      "                  'pageRank': 0.09995386179643245,\n",
      "                  'secLang': 'en',\n",
      "                  'secTitle': 'Moonsault',\n",
      "                  'secUrl': 'http://en.wikipedia.org/wiki/Moonsault',\n",
      "                  'support': [{'chFrom': 174,\n",
      "                               'chTo': 179,\n",
      "                               'entropy': 0,\n",
      "                               'pMentionGivenSurface': 3.023523009010099e-05,\n",
      "                               'pageRank': 3.466834856858538e-06,\n",
      "                               'prbConfidence': 1,\n",
      "                               'wFrom': 36,\n",
      "                               'wTo': 37}],\n",
      "                  'supportLen': 1,\n",
      "                  'title': 'Moonsault',\n",
      "                  'url': 'http://en.wikipedia.org/wiki/Moonsault',\n",
      "                  'wikiDataClasses': [],\n",
      "                  'wikiDataItemId': 'Q3323603'}],\n",
      " 'fnRegIdx': 'enwiki-20180220-pages-articles-index3.bin',\n",
      " 'fnWikiData': 'wikidata-20180226-all.bin',\n",
      " 'incIdxTimestampUtc': '',\n",
      " 'language': 'en',\n",
      " 'linkSourcesAvailable': True,\n",
      " 'maxDfThreshold': -1,\n",
      " 'minPageRank': 0.09995386179643245,\n",
      " 'normWords': ['Hey',\n",
      "               'smilin',\n",
      "               \"'\",\n",
      "               'strange',\n",
      "               \"You're\",\n",
      "               'lookin',\n",
      "               \"'\",\n",
      "               'happily',\n",
      "               'deranged',\n",
      "               'Could',\n",
      "               'you',\n",
      "               'settle',\n",
      "               'to',\n",
      "               'shoot',\n",
      "               'me',\n",
      "               '?',\n",
      "               'Or',\n",
      "               'have',\n",
      "               'you',\n",
      "               'picked',\n",
      "               'your',\n",
      "               'target',\n",
      "               'yet',\n",
      "               '?',\n",
      "               'Hey',\n",
      "               'Sandy',\n",
      "               \"Don't\",\n",
      "               'you',\n",
      "               'talk',\n",
      "               'back',\n",
      "               ',',\n",
      "               'Hey',\n",
      "               'Sandy',\n",
      "               'Four',\n",
      "               'feet',\n",
      "               'away',\n",
      "               'End',\n",
      "               'of',\n",
      "               'speech',\n",
      "               ',',\n",
      "               \"it's\",\n",
      "               'the',\n",
      "               'end',\n",
      "               'of',\n",
      "               'the',\n",
      "               'day',\n",
      "               'We',\n",
      "               'was',\n",
      "               'only',\n",
      "               'funnin',\n",
      "               \"'\",\n",
      "               'But',\n",
      "               'guiltily',\n",
      "               'I',\n",
      "               'thought',\n",
      "               'you',\n",
      "               'had',\n",
      "               'it',\n",
      "               'comin',\n",
      "               \"'\"],\n",
      " 'ranges': [],\n",
      " 'settings': {'altLabelSetsToUse': [],\n",
      "              'applyPageRankSqThreshold': True,\n",
      "              'candSelectionCosineWeight': 0,\n",
      "              'candSelectionLinkCtxCosineWeight': 0,\n",
      "              'candSelectionPageRankThreshold': 2,\n",
      "              'candSelectionUseLinearizedPageRank': False,\n",
      "              'candSelectionUseLinkProb': 'no',\n",
      "              'extraVocabsToInclude': [],\n",
      "              'maxMentionEntropy': 3,\n",
      "              'maxTargetsPerMention': 20,\n",
      "              'minLinkFrequency': 1,\n",
      "              'minLinkRelFrequency': 0,\n",
      "              'minPMentionGivenPhrase': 0,\n",
      "              'nTopDfWordsToIgnore': 0,\n",
      "              'nWordsToIgnoreFromList': 200,\n",
      "              'pageRankSqThreshold': 0.8,\n",
      "              'secondaryAnnotLanguage': 'en',\n",
      "              'semanticSimilarity': 'in',\n",
      "              'useLogLinkCounts': False},\n",
      " 'spaces': ['',\n",
      "            ' ',\n",
      "            '',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            '',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            '\\n',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            ' ',\n",
      "            '',\n",
      "            '\\n'],\n",
      " 'timeAnnotationMs': 41.282,\n",
      " 'timeBuildJsonMs': 0,\n",
      " 'timeTotalMs': 42.046,\n",
      " 'words': ['Hey',\n",
      "           'smilin',\n",
      "           \"'\",\n",
      "           'strange',\n",
      "           \"You're\",\n",
      "           'lookin',\n",
      "           \"'\",\n",
      "           'happily',\n",
      "           'deranged',\n",
      "           'Could',\n",
      "           'you',\n",
      "           'settle',\n",
      "           'to',\n",
      "           'shoot',\n",
      "           'me',\n",
      "           '?',\n",
      "           'Or',\n",
      "           'have',\n",
      "           'you',\n",
      "           'picked',\n",
      "           'your',\n",
      "           'target',\n",
      "           'yet',\n",
      "           '?',\n",
      "           'Hey',\n",
      "           'Sandy',\n",
      "           \"Don't\",\n",
      "           'you',\n",
      "           'talk',\n",
      "           'back',\n",
      "           ',',\n",
      "           'Hey',\n",
      "           'Sandy',\n",
      "           'Four',\n",
      "           'feet',\n",
      "           'away',\n",
      "           'End',\n",
      "           'of',\n",
      "           'speech',\n",
      "           ',',\n",
      "           \"it's\",\n",
      "           'the',\n",
      "           'end',\n",
      "           'of',\n",
      "           'the',\n",
      "           'day',\n",
      "           'We',\n",
      "           'was',\n",
      "           'only',\n",
      "           'funnin',\n",
      "           \"'\",\n",
      "           'But',\n",
      "           'guiltily',\n",
      "           'I',\n",
      "           'thought',\n",
      "           'you',\n",
      "           'had',\n",
      "           'it',\n",
      "           'comin',\n",
      "           \"'\"]}\n"
     ]
    }
   ],
   "source": [
    "pprint(WikifiedExample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Sandy (http://en.wikipedia.org/wiki/Hey_Sandy)\n",
      "Moonsault (http://en.wikipedia.org/wiki/Moonsault)\n"
     ]
    }
   ],
   "source": [
    "for annotation in WikifiedExample[\"annotations\"]:\n",
    "    print(\"%s (%s)\" % (annotation[\"title\"], annotation[\"url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. _(30 points)_ Link embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B1.__ _(5 points)_ To work with the Wikifier output we'll need to go through a few steps. Since the annotationd (embedded links) provided by Wikifer reference specific document indices, i.e., for words (of their tokenization) and characters. We'll work on the character level because it's more precise. To get this off of the ground, make a function called `build_doc(wiki)`, that takes a Wikifier output (`wiki`) and builds up the original document from the `wiki['words']` and `wiki['spaces']` fields. \\[__Hint.__ These two fields alternate,  with `wiki['spaces']` having exactly one more element than `wiki['words']`, i.e., a rebuilt document will join the elements of the two with the first element coming from `wiki['spaces']`\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc(wiki):\n",
    "    Original_Text = \"\"\n",
    "    for i in range(len(wiki['spaces'])):\n",
    "        Original_Text = Original_Text + wiki['spaces'][i]\n",
    "        if i<= len(wiki[\"words\"])-1:\n",
    "            Original_Text = Original_Text + wiki[\"words\"][i]\n",
    "        else:\n",
    "            continue\n",
    "    return Original_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Hey smilin' strange\\n\"\n",
      " \"You're lookin' happily deranged\\n\"\n",
      " 'Could you settle to shoot me?\\n'\n",
      " 'Or have you picked your target yet?\\n'\n",
      " 'Hey Sandy\\n'\n",
      " \"Don't you talk back,\\n\"\n",
      " 'Hey Sandy\\n'\n",
      " 'Four feet away\\n'\n",
      " \"End of speech, it's the end of the day\\n\"\n",
      " \"We was only funnin'\\n\"\n",
      " \"But guiltily I thought you had it comin'\\n\")\n"
     ]
    }
   ],
   "source": [
    "pprint(build_doc(WikifiedExample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ _(5 points)_ Now that the document is back together we want to collect the links from the annotations. Write a function called `get_links(wiki)` that processes the `annotation` objects from the `wiki['annotations']` field (a list of `annotation` objects), and creates/outputs a data object called links, which is a list of tuples:\n",
    "```\n",
    "links = [(chFrom, chTo, hyperlink), ...]\n",
    "```\n",
    "Hyperlinks will come from `annotation[\"url\"]`, and the `chFrom` and `chTo` elements represent character ranges corresponding to embeddings of `annotation[\"url\"]` in the document. Note that tthere are multiple character ranges: one for each `support` object in `annotation[\"support\"]` (also a list).  Specifically, each `support` object in the annotation field has two character indices keyed as `support[\"chFrom\"]` and `support[\"chTo\"]` which designate a range (in character indices) over which the hyperlink might well be embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(wiki):\n",
    "    links = []\n",
    "    for annotation in wiki[\"annotations\"]:\n",
    "        for eachsupport in annotation[\"support\"]:\n",
    "            links.append((eachsupport[\"chFrom\"], eachsupport[\"chTo\"], annotation[\"url\"]))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(118, 126, 'http://en.wikipedia.org/wiki/Hey_Sandy'),\n",
       " (149, 157, 'http://en.wikipedia.org/wiki/Hey_Sandy'),\n",
       " (174, 179, 'http://en.wikipedia.org/wiki/Moonsault')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(WikifiedExample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ _(5 points)_ Your next job is to create an `embed_link(doc, link)` function that takes the full document output from __B1__, and, a given link (list element) from __B2__ outputs an updated document with hyperlink embedded. Exhibit the function of `embed_link` using `IPython`'s `HTML()` function: `from IPython.core.display import HTML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlinker(doc, startchar_index, endchar_index, url):\n",
    "    link = doc[startchar_index:endchar_index+1]\n",
    "    hyperlink = \"<a href=\"+url+\">\"+link+\"</a>\"\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_link(doc, link):\n",
    "    start = 0\n",
    "    doc1 = \"\"\n",
    "    for eachlink in link:\n",
    "        doc1 = (doc1 + \n",
    "                doc[start:eachlink[0]] +\n",
    "                hyperlinker(doc, eachlink[0], eachlink[1], eachlink[2]))\n",
    "        start = eachlink[1]+1\n",
    "    doc2 = doc1+doc[start:]\n",
    "    return HTML(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Hey smilin' strange\n",
       "You're lookin' happily deranged\n",
       "Could you settle to shoot me?\n",
       "Or have you picked your target yet?\n",
       "<a href=http://en.wikipedia.org/wiki/Hey_Sandy>Hey Sandy</a>\n",
       "Don't you talk back,\n",
       "<a href=http://en.wikipedia.org/wiki/Hey_Sandy>Hey Sandy</a>\n",
       "Four feet away\n",
       "<a href=http://en.wikipedia.org/wiki/Moonsault>End of</a> speech, it's the end of the day\n",
       "We was only funnin'\n",
       "But guiltily I thought you had it comin'\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_link(build_doc(WikifiedExample), get_links(WikifiedExample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B4.__ _(5 points)_ The responses received from Wikifier are really only _predictions_ of where links should go. This means that multiple link predictions might span overlapping portions of source text. So, we'll have to make sure our code does some light decision making to avoid overlapping hyperlinks and broken html code. Review the Wikifier response data and discuss any viable approach to dealing with the possible overlapping link problem in the response box below. \\[__Hint__: One such approach is taken in problem __B5__ that you may discuss here. If you do so, you are required to provide a detailed description of _how_ __B5__ resolves the overlapping-link problem and any limitations of its approach.\\]\n",
    "\n",
    "__Note__: Even if you have not yet generated any Wikifier output through Problem A, you may complete this part for full credit using the sample data provided in `./data/example_annotations.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>__Approach:__ In order to avoid overlapping links, we could make a list of all the starting and stopping indices of the urls and check if this list is sorted in a stricktly increasing order. If the list is not increasing we may face the overlapping problem. If the list is monotonically increasing, we have two adjacent distinct links.</font><br ><br >\n",
    "<font color=blue>__Limitations:__ This approach prevents overlapping, but if the end index of one overlapping link is before the end index of the previous link the hyperlinks in the final document is not placed correct. I have shown one instance of this problem by feeding a faulty link list to the test function at the end of this section.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B5.__ _(10 points)_ Here is where we're going to build a wrapper function, called `embed_links()`, to manage the link embedding process, being sure to avoid the overlapping link problem. Specifically, your function _must_ do the following, in order:\n",
    "\n",
    "1. Take a wikifier, `wiki`, response as input.\n",
    "2. Build the document using __B1__ and storing the output as a string called `doc`\n",
    "3. Apply __B2__'s `get_links()` function, storing the `links` output \n",
    "4. _Reverse_ sort the `link`  by their ending points, i.e., `support[\"chTo\"]` values\n",
    "5. Loop through the _sorted_ `links` \n",
    "6. Conditionally use __B4__'s `embed_link()` function to embed links into `doc` in the loop. Specifically, only embed a link if its `support[\"chTo\"]` value is less than the previous one's `support[\"chFrom\"]` value, or the link to embed is the first in the loop.\n",
    "7. `return` the link-enhanced doc and exhibit it's output using `HTML()`.\n",
    "\n",
    "__Note__: If you have not yet completed __Problem A__, you can complete this part for full credit using the sample file, `./data/example_annotations.json`. This file has the correct input format of data for your function (a Wikifier json output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_links(wiki):\n",
    "    doc = build_doc(wiki)\n",
    "    links = get_links(wiki)\n",
    "    sorted_links = sorted(links, key=lambda tup: tup[1], reverse = True)\n",
    "    valid_links = []\n",
    "    for i in range(len(sorted_links)-1):\n",
    "        if sorted_links[i+1][1]<sorted_links[i][0]:\n",
    "            valid_links.append(sorted_links[i])\n",
    "    valid_links.append(sorted_links[-1])        \n",
    "    new_links = valid_links[::-1]\n",
    "    return embed_link(doc, new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Hey smilin' strange\n",
       "You're lookin' happily deranged\n",
       "Could you settle to shoot me?\n",
       "Or have you picked your target yet?\n",
       "<a href=http://en.wikipedia.org/wiki/Hey_Sandy>Hey Sandy</a>\n",
       "Don't you talk back,\n",
       "<a href=http://en.wikipedia.org/wiki/Hey_Sandy>Hey Sandy</a>\n",
       "Four feet away\n",
       "<a href=http://en.wikipedia.org/wiki/Moonsault>End of</a> speech, it's the end of the day\n",
       "We was only funnin'\n",
       "But guiltily I thought you had it comin'\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_links(WikifiedExample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPython.core.display.HTML"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed_links(WikifiedExample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Here, I want to make a temporary list of faulty links(Since I do not have a text sample with this problem, I am feeding a faulty link list to a test function similar to the one that we are using):</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "faultyLinklist = [(118, 126, 'http://en.wikipedia.org/wiki/Hey_Sandy'),\n",
    " (149, 157, 'http://en.wikipedia.org/wiki/Hey_Sandy'),\n",
    " (150, 155, 'http://en.wikipedia.org/wiki/Moonsault')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embed_links(wiki):\n",
    "    doc = build_doc(wiki)\n",
    "    links = faultyLinklist\n",
    "    sorted_links = sorted(links, key=lambda tup: tup[1], reverse = True)\n",
    "    valid_links = []\n",
    "    for i in range(len(sorted_links)-1):\n",
    "        if sorted_links[i+1][1]<sorted_links[i][0]:\n",
    "            valid_links.append(sorted_links[i])\n",
    "    valid_links.append(sorted_links[-1])        \n",
    "    new_links = valid_links[::-1]\n",
    "    return embed_link(doc, new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Hey smilin' strange\n",
       "You're lookin' happily deranged\n",
       "Could you settle to shoot me?\n",
       "Or have you picked your target yet?\n",
       "<a href=http://en.wikipedia.org/wiki/Hey_Sandy>Hey Sandy</a>\n",
       "Don't you talk back,\n",
       "H<a href=http://en.wikipedia.org/wiki/Moonsault>ey San</a>dy\n",
       "Four feet away\n",
       "End of speech, it's the end of the day\n",
       "We was only funnin'\n",
       "But guiltily I thought you had it comin'\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embed_links(WikifiedExample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It works  but it has a bug **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BONUS__ _(5 points)_ The `embed_links()` function's design overcomes an additional challenge. Your job here is to explain in detail how it does so. Here's the challenge: since we are using character indices to insert the links over ranges of the input document, adding the markup required for html _necessarily_ creates a longer document and throws off the original document's character indices from their original positions. How does __B5__'s design avoid this _index modification_ problem and simply utilize the exact character indices provided by Wikifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>This function uses the wiki instead of the html document. Wiki is a dictionary with words and spaces and the embed_links function rebuilds the document using the words and spaces. Therefore, the document that we work on is the raw text without any html markup. The indices extracted are based on the location of the hyperlinks in the pure text without any markup.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BONUS__ _(15 pts)_ Modify your code from __B5__ to handle the overlapping link (and potentially index modification) problem(s) differently and exhibit its output using the `HTML()` function. You may use any strategy distinct from those used in __B5__, but must _additionally_ (_for 5  of the __BONUS__ points_) describe sufficient details to explain your approach's distinctness, along with _how_ your approach satisfies these problems in the response below. \\[__Hint__. There's a useful field in the `support` objects held under the `\"pMentionGivenSurface\"` key; think thresholds and numerical comparisions, and modify the `get_links()` function to make this information available with each link.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>pMentionGivenSurface is the probability that, when a link appears in the Wikipedia with this particular subrange as its anchor text, it points to the Wikipedia page corresponding to the current annotation.\n",
    "We can extract this value for each link and if the value is more than a threshold, such as 1e-2, the link is probably reliable.<br ><br >\n",
    "As we expected, the faulty link \"moonsault\" for the phrase \"End of\" is removed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_get_links(wiki):\n",
    "    links = []\n",
    "    validlinks = []\n",
    "    for annotation in wiki[\"annotations\"]:\n",
    "        for eachsupport in annotation[\"support\"]:\n",
    "            links.append((\n",
    "                eachsupport[\"chFrom\"], eachsupport[\"chTo\"], annotation[\"url\"], eachsupport['pMentionGivenSurface']))\n",
    "    for eachlink in links:\n",
    "        if eachlink[3]>=1e-2:\n",
    "            validlinks.append(eachlink)\n",
    "        else:\n",
    "            pass\n",
    "    return validlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(118, 126, 'http://en.wikipedia.org/wiki/Hey_Sandy', 0.3846153846153846),\n",
       " (149, 157, 'http://en.wikipedia.org/wiki/Hey_Sandy', 0.3846153846153846)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_get_links(WikifiedExample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. _(25 points)_ Modifying html documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C1.__ _(10 points)_ Write a function called `wikify_html(path)` that does the following: \n",
    "1. Takes an local html file's path as input,\n",
    "2. uses `BeautifulSoup` to loop over the `paragraph`s as strings, i.e., the string objects returned from the `.text` attributes of `<p>...</p>` tagged blocks, and\n",
    "3. applies Wikifier to each,\n",
    "4. writing the json annotation results, line by line (i.e., append mode) to a file with same path as the input file with the following suffix (extension) added: '.json'.\n",
    "\n",
    "_Note_: If you have not completed __A2&mdash;A3__, you may complete this entire section for full credit using the packaged file: \n",
    "- `./data/example.htm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAlice = \"./BookID19033.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikify_html(path):\n",
    "    with open(path, \"r\") as f: \n",
    "        html_file = f.read()\n",
    "    soup = BeautifulSoup(html_file, 'html.parser')\n",
    "    ptags = soup.find_all('p')\n",
    "    annot = []\n",
    "    for ptag in ptags:\n",
    "        annot.append(Wikify(ptag.text))\n",
    "    with open(path[:-4]+\".json\", 'w') as f:\n",
    "        json.dump(annot, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "wikify_html(pathAlice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ _(15 points)_ Finally (and this is a big one), we'd like to replace html paragraphs with ones that we've annotated with the URLs pointing to the Wikipedia articles. To do so, write a function called `get_enhanced_html(path)` that does the following:\n",
    "\n",
    "1. Takes an html document's path as input,\n",
    "2. checks to see if existing annotation data are already stored for the file (having the additional .json extension), \n",
    "    - running `wikify_html(path)` (from __C1__) if no annotations exist\n",
    "3. loads the html document's `annotations` from file\n",
    "4. retrieves the correct (by index) `annotation` from the `annotations` list,\n",
    "5. uses `BeautifulSoup` to loop over the `paragraph`s, i.e., the `<p>...</p>` tagged blocks with the `enumerate()` function (to capture indices),\n",
    "6. passes the `annotation` object through the `embed_links()` function, naming the output `annotated_paragraph_text`,\n",
    "7. encloses `annotated_paragraph_text` in `<p>` tags and then applies `BeautifulSoup()`, storing the interpreted object as the `enhanced_paragraph`,\n",
    "8. replaces each original `paragraph` with its corresponding `enhanced_paragraph` using the `.replace_with()` method for `BeautifulSoup` objects, and \n",
    "9. after the loop finishes, returns the enhanced html document (`BeautifulSoup` object).\n",
    "\n",
    "Additionally, when done exhibit your function's output from application to the file under the path: `./data/example.htm`.\n",
    "\n",
    "\\[__Hint__: For Step 2, use the `os` module's function: `os.path.exists(path)`.\\] \n",
    "\n",
    "__Note__: For more information on the `.replace_with()` method, check out the documentation at:\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/#replace-with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_html(path):\n",
    "    if os.path.exists(path[:-4]+\".json\"):\n",
    "        with open(path[:-4]+\".json\") as fh:\n",
    "            Wikis = json.load(fh) #step3\n",
    "    else:\n",
    "        wikify_html(path)\n",
    "        with open(path[:-4]+\".json\") as fh:\n",
    "            Wikis = json.load(fh) #step3\n",
    "############################################# \n",
    "    with open(path, \"r\") as f: \n",
    "        html_file = f.read()\n",
    "    originalsoup = BeautifulSoup(html_file, 'html.parser')\n",
    "    ptags = originalsoup.find_all('p')\n",
    "    ptags_index_tuples = [(ptag, i) for i, ptag in enumerate(ptags)] #step 5\n",
    "############################################# \n",
    "    annotations_and_num = [(wiki, len(wiki['annotations'])) for i, wiki in enumerate(Wikis)]\n",
    "############################################# \n",
    "    annotated_paragraph_text = []\n",
    "    for eachtuple in annotations_and_num:\n",
    "        if eachtuple[1]:\n",
    "            annotated_paragraph_text.append(embed_links(eachtuple[0]).data)\n",
    "        else:\n",
    "            annotated_paragraph_text.append(HTML(build_doc(eachtuple[0])).data)\n",
    "############################################# \n",
    "    enhanced_paragraph = []\n",
    "    for eachpar in annotated_paragraph_text:\n",
    "        htmlpar = \"<p>\"+eachpar+\"</p>\"\n",
    "        enhanced_paragraph.append(BeautifulSoup(htmlpar, 'html.parser')) #end of step 7\n",
    "#############################################\n",
    "    i = 0\n",
    "    for p in originalsoup.find_all('p'):\n",
    "        newparagraph = enhanced_paragraph[i]\n",
    "        if p.string :\n",
    "            p.string.replace_with(newparagraph)\n",
    "        else:\n",
    "            continue\n",
    "        i += 1\n",
    "        if i == len(ptags)-1:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    with open(path[:-4]+\"Enhanced.html\", 'wb') as f:\n",
    "        f.write(originalsoup.renderContents())\n",
    "#############################################\n",
    "    with open(path[:-4]+\"Enhanced.html\", \"r\") as f: \n",
    "        enhanced_html_file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"200\"\n",
       "            src=\"./BookID19033Enhanced.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x115aec160>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_enhanced_html(pathAlice)\n",
    "IFrame(pathAlice[:-4]+\"Enhanced.html\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. _(25 points)_ Packaging and distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D1.__  _(5 points)_ From a efficient access and distribution perspective, explain why __C1__'s Step 4 and __C2__'s Step 2 make this tool's processing minimal, and most of all, nice to the servers responding to our requests. Record this discussion in the response box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Since a book has lots of paragraghs and it takes a while to wikify each paragragh seperately, if we have a way to run this part only once it would save a lot of computational power. Step 4 of C1 saves the results for future use and step 2 of C1 checks if the wikifier was already ran on the book, it will not produce the results again and only loads the file that has been saved.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D2.__ _(10 points)_ For distribution, it's important to have all your code packaged up nice and neat. So, take everything you've done in the previous steps and collect it all and place it so that it can be run with a single function call. Specifically create a function named `enhanced_book(book_id)` that does the following:\n",
    "1. Inputs a `book_id`,\n",
    "2. checks to see the book's file path exists locally, \n",
    "    - downloading an html copy using __A2__'s `download_book(book_id)` and storing locally if no local copy exists,\n",
    "3. runs __C2__'s `get_enhanced_html(path)` function,\n",
    "4. finally returning Step 3's `enhanced_html` output, exhibiting function by coercing the output to a string and applying the `HTML()` function.\n",
    "\n",
    "__Important__: Completing the application of the full pipeline to an entire book from Project Gutenberg takes a long time (about 20-30 minutes for _Alice in Wonderland_). Make sure you test your code for function carefully, only when you're fairly sure it's completely done. Full credit will be awarded to attempts that have not been completely implemented, so if you run out of time for full testing please submit code anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_book(book_id):\n",
    "    path = \"./BookID\"+book_id+\".htm\"\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        save_html_file(book_id)\n",
    "    get_enhanced_html(path)\n",
    "    return IFrame(path[:-4]+\"Enhanced.html\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"200\"\n",
       "            src=\"./BookID19033Enhanced.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11884cdd8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_book(\"19033\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D3.__ _(5 points)_ Now that you have a functioning and self-contained data pipeline, think of possible models for distribution of said data. Describe the data distribution model you believe to be the best. Should the software download and store all data locally on a user's machine, or should it always access data on the fly from the web?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Downloading the whole data is not a very promising approach since it uses a lot of memory. Instead it is best to make a request for each data seperatly when they are needed. We can define a parent function which inputs the ID of the book and downloads the html file and calls all the related functions in order to produce the enhanced ebook. This function will save the nessesary files in between to avoid duplicate calculations. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D4.__ _(5 points)_ Finally, compare the possible advantages and weaknesses of each of the data distribution models you considered in the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>This approach makes a user friendly setup for creating the enhanced ebook and does not use a lot of memory on the user's machine. But on the other hand, it takes time to run the code for a whole book.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
